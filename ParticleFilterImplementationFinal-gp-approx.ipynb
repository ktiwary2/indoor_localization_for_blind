{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from copy import deepcopy\n",
    "import scipy\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(bp104_1)\n",
    "# print(bp104_2)\n",
    "# print(bp104_3)\n",
    "# print(bp104_4)\n",
    "\n",
    "#             for i in range (len(measurments)):\n",
    "#                 like_ = scipy.stats.norm.pdf(z_t[i], measurments[i][0].flatten(), measurments[i][1].flatten())\n",
    "#                 likelihood*=like_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "create_singular_model: if True will creaet \n",
    "'''\n",
    "class MeasurmentModel:\n",
    "    def __init__(self, map_, bp104s, normed_consts, var_const = 1, create_singular_model = False):\n",
    "        self.map_ = map_\n",
    "        self.bp104_1, self.bp104_2, self.bp104_3, self.bp104_3 = bp104s \n",
    "        self.normed_consts = normed_consts\n",
    "        self.gp_models = None\n",
    "        self.var_const = var_const\n",
    "        self.kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "        self.create_singular_model = create_singular_model\n",
    "        self.bp104_1_loc = [0.5, 2.5]\n",
    "        self.bp104_2_loc = [16.5,9.5]\n",
    "        self.bp104_3_loc = [18.5,2.5]\n",
    "        self.bp104_4_loc = [10.5,4.5]\n",
    "        \n",
    "        if self.create_singular_model:\n",
    "            # create one model for all beacons\n",
    "            print(\"Creating one gp model for all beacons.\")\n",
    "            self.gp_models = self._create_singular_gp_model()\n",
    "        else:\n",
    "            # Create four models - one per beacon\n",
    "            print(\"Creating four gp model for each beacons.\")\n",
    "            self.gp_models = []\n",
    "            self._create_four_gp_models()\n",
    "            \n",
    "    \"\"\"\n",
    "    Function gets liklihood of observing z_t at state_t based on the \n",
    "    GP regression model \n",
    "    z_t: actual measurment at time t\n",
    "    state_t: state of the particle at time t\n",
    "    \"\"\"\n",
    "    def get_likelihood(self, z_t, state_t):\n",
    "        if self.create_singular_model:\n",
    "            mu, sigma = self.predict(state_t)\n",
    "            l1, l2, l3, l4 = scipy.stats.norm.pdf(z_t, mu.flatten(), sigma.flatten())\n",
    "            likelihood = l1 * l2 * l3 * l4\n",
    "            if likelihood == 0:\n",
    "                return 1.e-300\n",
    "            else:\n",
    "                print('final_likelihood', likelihood)\n",
    "                return likelihood\n",
    "        else:\n",
    "            measurments = self.predict(state_t)\n",
    "            likelihood = 1.0\n",
    "            likelihood_ = scipy.stats.norm.pdf(z_t, measurments[:,0], np.diag(measurments[:,1]))\n",
    "            for i in range (likelihood_.shape[0]):\n",
    "                likelihood = likelihood *  likelihood_[i,i]\n",
    "            if likelihood == 0:\n",
    "                return 1.e-300 # avoid norm. problems\n",
    "            else:\n",
    "                return likelihood\n",
    "        \n",
    "    def predict(self, state):\n",
    "        if self.create_singular_model:\n",
    "            mu, sigma = self.gp_models.predict([[state[0], state[1]]], return_cov=True)\n",
    "            return [mu, sigma]\n",
    "        else:\n",
    "            measurment = []\n",
    "            for i in range(len(self.gp_models)):\n",
    "                mu, sigma = self.gp_models[i].predict([[state[0], state[1]]], return_cov=True)\n",
    "                measurment.append(np.array([mu.flatten()[0], sigma.flatten()[0]]))\n",
    "            return np.array(measurment)\n",
    "\n",
    "    def _get_beacon_dists(self, state):\n",
    "        bp104_1_dist = np.sqrt((state[0]-self.bp104_1_loc[0])**2 + (state[1] - self.bp104_1_loc[1])**2)\n",
    "        bp104_2_dist = np.sqrt((state[0]-self.bp104_2_loc[0])**2 + (state[1] - self.bp104_2_loc[1])**2)\n",
    "        bp104_3_dist = np.sqrt((state[0]-self.bp104_3_loc[0])**2 + (state[1] - self.bp104_3_loc[1])**2)\n",
    "        bp104_4_dist = np.sqrt((state[0]-self.bp104_4_loc[0])**2 + (state[1] - self.bp104_4_loc[1])**2)\n",
    "\n",
    "        dist_arr = np.array([bp104_1_dist, bp104_2_dist, bp104_3_dist, bp104_4_dist])\n",
    "        dist_arr = 1./dist_arr\n",
    "        normed_dist_arr = dist_arr / np.sum(dist_arr)\n",
    "        return normed_dist_arr\n",
    "    \n",
    "    def _create_four_gp_models(self):\n",
    "        self.gp_models.append(self._create_one_beacon_model(bp104_1))\n",
    "        self.gp_models.append(self._create_one_beacon_model(bp104_2))\n",
    "        self.gp_models.append(self._create_one_beacon_model(bp104_3))\n",
    "        self.gp_models.append(self._create_one_beacon_model(bp104_4))\n",
    "        print(\"Created 4 GP models\")\n",
    "        \n",
    "    def _create_singular_gp_model(self):\n",
    "        for i in range (bp104_1.shape[0]):\n",
    "            for j in range (bp104_1.shape[1]):    \n",
    "                X.append(np.array([i,j])) # just the state [0,0], [0,1] etc... \n",
    "                y.append(np.array([bp104_1[i,j], bp104_2[i,j], bp104_3[i,j], bp104_4[i,j]]))\n",
    "        X= np.array(X)\n",
    "        y= np.array(y)\n",
    "        dy = np.random.random(y.shape) * self.var_const\n",
    "        # Instantiate a Gaussian Process model\n",
    "        gp = GaussianProcessRegressor(kernel=self.kernel,normalize_y= True, alpha = dy,\n",
    "                                      n_restarts_optimizer=10)\n",
    "        \n",
    "        # Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "        gp.fit(X,y)\n",
    "        return gp\n",
    "    \n",
    "    def _create_one_beacon_model(self, bp104_x):\n",
    "        X = []\n",
    "        y = []\n",
    "        for i in range (bp104_x.shape[0]):\n",
    "            for j in range (bp104_x.shape[1]):    \n",
    "                X.append(np.array([i,j])) # just the state [0,0], [0,1] etc... \n",
    "                y.append(bp104_x[i,j])\n",
    "\n",
    "        X= np.array(X)\n",
    "        y= np.array(y)\n",
    "        dy = np.random.random(y.shape)* self.var_const\n",
    "        # Instantiate a Gaussian Process model\n",
    "        gp_bp104x = GaussianProcessRegressor(kernel=self.kernel, alpha=dy,normalize_y= True,\n",
    "                                      n_restarts_optimizer=10)\n",
    "\n",
    "        # Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "        gp_bp104x.fit(X,y)\n",
    "        return gp_bp104x \n",
    "    \n",
    "    def get_normalized_values(self, measurment):\n",
    "        #measurment should be four values \n",
    "        return np.array(measurment) - np.array(self.normed_consts)\n",
    "    \n",
    "    def return_gp_models(self):\n",
    "        return self.gp_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-41 -44 -44 -46]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_ = np.load(\"/Users/kushagratiwary/Documents/ECE445/map_data/final_041719_0112/map_binary_.npy\")\n",
    "bp104_1 = np.load(\"/Users/kushagratiwary/Documents/ECE445/map_data/final_041719_0112/bp104_1.npy\")\n",
    "bp104_2 = np.load(\"/Users/kushagratiwary/Documents/ECE445/map_data/final_041719_0112/bp104_2.npy\")\n",
    "bp104_3 = np.load(\"/Users/kushagratiwary/Documents/ECE445/map_data/final_041719_0112/bp104_3.npy\")\n",
    "bp104_4 = np.load(\"/Users/kushagratiwary/Documents/ECE445/map_data/final_041719_0112/bp104_4.npy\")\n",
    "norm_constants = np.load(\"/Users/kushagratiwary/Documents/ECE445/map_data/final_041719_0112/norm_constants.npy\")\n",
    "\n",
    "\n",
    "measurments = [\n",
    "    [-51.,-80.,-66.,-64.],\n",
    "    [-53.,-80.,-67.,-62.],\n",
    "\n",
    "    [-59.,-80.,-67.,-62.],\n",
    "    [-57.,-80.,-64.,-64],\n",
    "    \n",
    "    [-58.,-66.,-61.,-52.],\n",
    "    [-58.,-63.,-63.,-52.],\n",
    "    \n",
    "    [-65.,-59.,-56.,-60.],\n",
    "    [-63.,-62.,-61.,-59.],\n",
    "    \n",
    "    [-66.,-62.,-52.,-61.],\n",
    "    [-65.,-58.,-52.,-61.],\n",
    "    \n",
    "    [-80.,-52.,-54.,-66.],\n",
    "    [-80.,-51.,-55.,-66.],\n",
    "\n",
    "    [-67.,-57.,-58.,-64.],\n",
    "    [-68.,-60.,-57.,-62.]\n",
    "]\n",
    "\n",
    "simulated_measurments = np.array(measurments).astype(float)\n",
    "print(norm_constants)\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating four gp model for each beacons.\n",
      "Created 4 GP models\n"
     ]
    }
   ],
   "source": [
    "GP_measurment_model = MeasurmentModel(map_, np.array([bp104_1, bp104_2, bp104_3, bp104_4]), \n",
    "                                      norm_constants, var_const = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gp_models = GP_measurment_model.return_gp_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/kushagratiwary/Documents/ECE445/_gp_approx_data/final_/map1/_x1015_y0510/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------i,j-------- 10 5\n",
      "----------i,j-------- 10 6\n",
      "----------i,j-------- 10 7\n",
      "----------i,j-------- 10 8\n",
      "----------i,j-------- 10 9\n",
      "----------i,j-------- 11 5\n",
      "----------i,j-------- 11 6\n",
      "----------i,j-------- 11 7\n",
      "----------i,j-------- 11 8\n",
      "----------i,j-------- 11 9\n",
      "----------i,j-------- 12 5\n",
      "----------i,j-------- 12 6\n",
      "----------i,j-------- 12 7\n",
      "----------i,j-------- 12 8\n",
      "----------i,j-------- 12 9\n",
      "----------i,j-------- 13 5\n",
      "----------i,j-------- 13 6\n",
      "----------i,j-------- 13 7\n",
      "----------i,j-------- 13 8\n",
      "----------i,j-------- 13 9\n",
      "----------i,j-------- 14 5\n",
      "----------i,j-------- 14 6\n",
      "----------i,j-------- 14 7\n",
      "----------i,j-------- 14 8\n",
      "----------i,j-------- 14 9\n"
     ]
    }
   ],
   "source": [
    "_x05y05_b1_mean = np.zeros((50,50))\n",
    "_x05y05_b2_mean = np.zeros((50,50))\n",
    "_x05y05_b3_mean = np.zeros((50,50))\n",
    "_x05y05_b4_mean = np.zeros((50,50))\n",
    "\n",
    "_x05y05_b1_cov = np.zeros((50,50))\n",
    "_x05y05_b2_cov = np.zeros((50,50))\n",
    "_x05y05_b3_cov = np.zeros((50,50))\n",
    "_x05y05_b4_cov = np.zeros((50,50))\n",
    "\n",
    "for i in range (10,15):\n",
    "    for j in range (5,10):\n",
    "        print('----------i,j--------', i,j)\n",
    "        for k in range (10):\n",
    "            for l in range (10):\n",
    "                idx_x = int((i%5)*10 + k)\n",
    "                idx_y = int((j%5)*10 + l)\n",
    "#                 print('k,l', k,l)\n",
    "#                 print('idx_x', idx_x)\n",
    "#                 print('idx_y', idx_y)\n",
    "#                 print(\"state:\", [i+(k/10.0), j+(l/10.0)])\n",
    "                mub1, cov1 = gp_models[0].predict([[i+(k/10.0), j+(l/10.0)]], return_cov=True)\n",
    "                mub2, cov2 = gp_models[1].predict([[i+(k/10.0), j+(l/10.0)]], return_cov=True)\n",
    "                mub3, cov3 = gp_models[2].predict([[i+(k/10.0), j+(l/10.0)]], return_cov=True)\n",
    "                mub4, cov4 = gp_models[3].predict([[i+(k/10.0), j+(l/10.0)]], return_cov=True)\n",
    "                _x05y05_b1_mean[idx_x, idx_y] = np.around(mub1,2)\n",
    "                _x05y05_b2_mean[idx_x, idx_y] = np.around(mub2,2)\n",
    "                _x05y05_b3_mean[idx_x, idx_y] = np.around(mub3,2)                \n",
    "                _x05y05_b4_mean[idx_x, idx_y] = np.around(mub4,2)                \n",
    "\n",
    "                _x05y05_b1_cov[idx_x, idx_y] = np.around(cov1,2)\n",
    "                _x05y05_b2_cov[idx_x, idx_y] = np.around(cov2,2)\n",
    "                _x05y05_b3_cov[idx_x, idx_y] = np.around(cov3,2)                \n",
    "                _x05y05_b4_cov[idx_x, idx_y] = np.around(cov4,2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# np.savetxt(path+'b1_mean.csv', _x05y05_b1_mean, delimiter=\",\", fmt='%1.2f')\n",
    "# np.savetxt(path+'b2_mean.csv', _x05y05_b2_mean, delimiter=\",\", fmt='%1.2f')\n",
    "# np.savetxt(path+'b3_mean.csv', _x05y05_b3_mean, delimiter=\",\", fmt='%1.2f')\n",
    "# np.savetxt(path+'b4_mean.csv', _x05y05_b4_mean, delimiter=\",\", fmt='%1.2f')\n",
    "\n",
    "# np.savetxt(path+'b1_cov.csv', _x05y05_b1_cov, delimiter=\",\", fmt='%1.2f')\n",
    "# np.savetxt(path+'b2_cov.csv', _x05y05_b2_cov, delimiter=\",\", fmt='%1.2f')\n",
    "# np.savetxt(path+'b3_cov.csv', _x05y05_b3_cov, delimiter=\",\", fmt='%1.2f')\n",
    "# np.savetxt(path+'b4_cov.csv', _x05y05_b4_cov, delimiter=\",\", fmt='%1.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# path = \"/Users/kushagratiwary/Documents/ECE445/_gp_approx_data/\"\n",
    "# # path = path+'non_truncated/'\n",
    "# all_dirs = os.listdir(path)\n",
    "# for dir_ in all_dirs:\n",
    "#     if (dir_[0] != '.'):\n",
    "#         files = os.listdir(path+dir_)\n",
    "#         for file_ in files:\n",
    "#             if file_ == '.DS_Store':\n",
    "#                 continue\n",
    "#             print(\"at:\", dir_)\n",
    "#             file_name = os.path.join(path+dir_,file_)\n",
    "#             print(\"Currently processing:\", file_name)\n",
    "#             array = genfromtxt(file_name, delimiter=',')\n",
    "#             array_new = np.around(array, decimals=2)\n",
    "#             new_direc = os.path.join(path+dir_+\"_new_direc\")\n",
    "# #             print(\"new directory create:\", new_file_name)\n",
    "#             if not os.path.exists(new_direc):\n",
    "#                 os.makedirs(new_direc)\n",
    "# #             print(\"new file save used at:\", os.path.join(new_direc,file_))\n",
    "# #             np.savetxt(os.path.join(new_direc,file_), array_new, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "# my_data = genfromtxt('my_file.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = genfromtxt('/Volumes/NO NAME/map1/_x1015_y1015/b2_cov.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/NO NAME/map1/\"\n",
    "path2= \"/Volumes/NO NAME/map2/\"\n",
    "\n",
    "all_dirs = os.listdir(path)\n",
    "for dir_ in all_dirs:\n",
    "    if (dir_[0] != '.'):\n",
    "        files = os.listdir(path+dir_)\n",
    "        for file_ in files:\n",
    "            if file_ == '.DS_Store':\n",
    "                continue\n",
    "            print(\"at:\", dir_)\n",
    "            file_name = os.path.join(path+dir_,file_)\n",
    "            print(\"Currently processing:\", file_name)\n",
    "            array = genfromtxt(file_name, delimiter=',')\n",
    "            new_save_dir = os.path.join(path2,dir_)\n",
    "            if not os.path.exists(new_save_dir):\n",
    "                os.makedirs(new_save_dir)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-b4a62b32d574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Volumes/NO NAME/map2/asd.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "with open('/Volumes/NO NAME/map2/asd.csv','wb') as fp:\n",
    "    a = csv.writer(fp,delimiter = \",\")\n",
    "    a.writerows(map(lambda t: (t[0], \"%.2f\" % t[1]), my_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-5.82965482]), array([[1.95513181]]))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_models[0].predict([[0.9, 4.10]], return_cov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-35.71507521]), array([[0.96232942]]))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_models[1].predict([[0.9560690629980964, 4.103558298359111]], return_cov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-19.84738947]), array([[2.42204413]]))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_models[2].predict([[0.9560690629980964, 4.103558298359111]], return_cov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-11.39007724]), array([[2.79656999]]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_models[3].predict([[0.9560690629980964, 4.103558298359111]], return_cov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pt = \"/Users/kushagratiwary/Documents/ECE445/_gp_approx_data/final_/map1/_x05_y05/b1_mean.csv\"\n",
    "# pt = \"/Users/kushagratiwary/Documents/ECE445/_gp_approx_data/final_/map1/_x05_y05/b1_cov.csv\"\n",
    "\n",
    "my_data = genfromtxt(pt, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.83"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data[9,41]\n",
    "# np.where(my_data == 1.583)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
